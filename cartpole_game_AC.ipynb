{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cartpole-game.py__________Actor-Critic__________CartPole-v0\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "\n",
    "np.random.seed(2)\n",
    "torch.manual_seed(2)  # reproducible\n",
    "\n",
    "\n",
    "\n",
    "# 检查是否有可用的GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_GRAPH = False\n",
    "MAX_EPISODE = 500\n",
    "DISPLAY_REWARD_THRESHOLD = 200  # renders environment if total episode reward is greater then this threshold\n",
    "MAX_EP_STEPS = 1000   # maximum time step in one episode\n",
    "RENDER = False\n",
    "GAMMA = 0.9     # reward discount in TD error\n",
    "LR_A = 0.001    # learning rate for actor\n",
    "LR_C = 0.01     # learning rate for critic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "env.reset(seed=1)  # reproducible\n",
    "\n",
    "N_F = env.observation_space.shape[0]\n",
    "N_A = env.action_space.n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, n_features, n_actions, lr=0.001):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_features, 32)\n",
    "        self.fc2 = nn.Linear(32, n_actions)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.softmax(self.fc2(x), dim=-1)\n",
    "        return x\n",
    "\n",
    "    def learn(self, s, a, td_error):\n",
    "        self.optimizer.zero_grad()\n",
    "        s = torch.tensor(s, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        a = torch.tensor(a, dtype=torch.int64).to(device)\n",
    "        td_error = torch.tensor(td_error, dtype=torch.float32).to(device)\n",
    "        probs = self.forward(s)\n",
    "        log_prob = torch.log(probs.squeeze(0)[a])\n",
    "        loss = -log_prob * td_error  # negative for gradient ascent\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        s = torch.tensor(s, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        probs = self.forward(s)\n",
    "        m = torch.distributions.Categorical(probs)\n",
    "        return m.sample().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, n_features, lr=0.01):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_features, 32)\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def learn(self, s, r, s_):\n",
    "        self.optimizer.zero_grad()\n",
    "        s = torch.tensor(s, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        s_ = torch.tensor(s_, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        r = torch.tensor(r, dtype=torch.float32).to(device)\n",
    "        v = self.forward(s)\n",
    "        v_ = self.forward(s_).detach()  # Detach v_ from the computation graph\n",
    "        td_error = r + GAMMA * v_ - v\n",
    "        loss = td_error.pow(2)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return td_error.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create an animation from frames\n",
    "def create_animation(frames):\n",
    "    fig = plt.figure()\n",
    "    plt.axis('off')\n",
    "    patch = plt.imshow(frames[0])\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, animate, frames=len(frames), interval=50)\n",
    "    plt.show(ani)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor(n_features=N_F, n_actions=N_A, lr=LR_A).to(device)\n",
    "critic = Critic(n_features=N_F, lr=LR_C).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "frames=[]\n",
    "flag=0\n",
    "for i_episode in range(MAX_EPISODE):\n",
    "    s, _ = env.reset()\n",
    "    t = 0\n",
    "    track_r = []\n",
    "    while True:\n",
    "        if RENDER and flag==1: \n",
    "            frame=env.render()\n",
    "            frames.append(frame)\n",
    "            print(f\"{i_episode} frame saved\")\n",
    "            flag=2\n",
    "        a = actor.choose_action(s)\n",
    "        s_, r, done, _, _ = env.step(a)\n",
    "        if done: r = -20\n",
    "        track_r.append(r)\n",
    "        td_error = critic.learn(s, r, s_)\n",
    "        actor.learn(s, a, td_error)\n",
    "        s = s_\n",
    "        t += 1\n",
    "        if done or t >= MAX_EP_STEPS:\n",
    "            ep_rs_sum = sum(track_r)\n",
    "            if 'running_reward' not in globals():\n",
    "                running_reward = ep_rs_sum\n",
    "            else:\n",
    "                running_reward = running_reward * 0.95 + ep_rs_sum * 0.05\n",
    "            if running_reward > DISPLAY_REWARD_THRESHOLD and flag==0 and i_episode>150: \n",
    "                RENDER = True\n",
    "                flag=1\n",
    "            print(\"episode:\", i_episode, \"  reward:\", int(running_reward))\n",
    "            rewards.append(running_reward)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 显示训练后的小车动画\n",
    "create_animation(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 绘制累积奖励随迭代次数的变化曲线\n",
    "plt.plot(rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Running Reward')\n",
    "plt.title('Running Reward vs Episode')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
